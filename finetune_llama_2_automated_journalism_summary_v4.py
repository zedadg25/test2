# -*- coding: utf-8 -*-
"""Finetune Llama-2 Automated Journalism  Summary- v4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FD_X6t8DhGL-bYqH3tpRhm22yTYpLOZx

https://github.com/brevdev/notebooks/blob/main/llama2-finetune.ipynb

# Install
"""

!pip install -q wandb -U

! pip3 install transformers>=4.32.0 optimum>=1.12.0
! pip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  # Use cu117 if on CUDA 11.7

# !pip install fastapi kaleido  python-multipart uvicorn

# !pip install typing-extensions==4.6.0

# You only need to run this once per machine
!pip install -q -U bitsandbytes
# !pip install -q -U git+https://github.com/huggingface/peft.git
# !pip install -q -U git+https://github.com/huggingface/accelerate.git
# !pip install -q -U datasets scipy ipywidgets matplotlib

!pip install evaluate nltk rouge_score

!pip install datasets

! pip install trl==0.4.7

"""# Imports"""

import pandas as pd
import torch
from torch.utils.data import DataLoader
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,AutoModelForSeq2SeqLM

from trl import SFTTrainer

"""# Config"""

from google.colab import drive
drive.mount('/content/gdrive')

class config:
  MODEL_NAME = "meta-llama/Llama-2-7b-chat-hf"

  DEVICE = torch.device("cuda:0") if torch.cuda.is_available() else torch.device("cpu")
  DATASET_NAME = 'readerbench/ro-text-summarization'

  ROOT_DIR = '/content/gdrive/MyDrive/Colab Notebooks/automated_journalism/llama_models'
  MAX_SOURCE_LEN = 512
  MAX_TARGET_LEN = 128

  PROMPT = """ Below is an article in Romanian language.
  ###Text: {article}

  ###Instruction:  Write the summary in Romanian language.

  ###Response:
  """

from google.colab import userdata
from huggingface_hub import hf_hub_download
from huggingface_hub import login

hf_token = userdata.get('HF_TOKEN')
login(token=hf_token)

config_model = {
  "lr": 3e-4,
  "epochs":3,
  "checkpoint": config.MODEL_NAME,
  "batch_size": 4,
  "prompt": config.PROMPT,
  "MAX_SOURCE_LEN": config.MAX_SOURCE_LEN,
  "MAX_TARGET_LEN": config.MAX_TARGET_LEN,
  "eps":1e-8,
  "do_sample": True,
  "temperature":0.6,
  "top_p":0.65,
  "top_k":40,
  "repetition_penalty":1.1,
  "model_name_saved": "story_llama_7b"
}

project = "rosummary_4"
base_model_name = config.MODEL_NAME
run_name = base_model_name + "-" + project
output_dir = config.ROOT_DIR +'/'+ run_name

from huggingface_hub import notebook_login
notebook_login()

"""# Read data"""

train_ds = load_dataset(config.DATASET_NAME, split="train[:50000]")

ds = train_ds.train_test_split(test_size=0.1)
train_ds = ds['train']
eval_ds = ds['test']

print(train_ds,eval_ds)

train_ds.to_pandas()

train_ds = train_ds.remove_columns(["Title","href","Source","Category"])
eval_ds = eval_ds.remove_columns(["Title","href","Source","Category"])
train_ds.to_pandas()

"""# Acc"""

# from accelerate import FullyShardedDataParallelPlugin, Accelerator
# from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig

# fsdp_plugin = FullyShardedDataParallelPlugin(
#     state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),
#     optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),
# )

# accelerator = Accelerator(fsdp_plugin=fsdp_plugin)

"""# Load base model"""

bnb_config = BitsAndBytesConfig(
      load_in_4bit=True,
      bnb_4bit_quant_type="nf4",
      bnb_4bit_compute_dtype=torch.float16,
  )

model_auto = AutoModelForCausalLM.from_pretrained(config.MODEL_NAME,
                                             device_map="auto",
                                             trust_remote_code=True,
                                            quantization_config=bnb_config,
                                            )

"""# Tokenize"""

tokenizer = AutoTokenizer.from_pretrained(
    config.MODEL_NAME,
    padding_side="right",
    add_eos_token=True,
    add_bos_token=True,
    use_fast=True
)
tokenizer.pad_token = tokenizer.eos_token

import re
def clean_text(text):
  text = re.sub(r"http\S+", "", text)
  text = re.sub(r"@[^\s]+", "", text)
  text = re.sub(r"\s+", " ", text)
  return re.sub(r"\^[^ ]+", "", text)

config.PROMPT

def get_prompt(data):
  prompt = config.PROMPT
  clean_data = clean_text(data['Content'])
  data['text'] = prompt.format(article=clean_data)
  return data

# We drop instances with more than 512 tokens for GOOD memory usage
def filter_by_text_length_content(data):
  return len(tokenizer.encode(data['Content'])) < config.MAX_SOURCE_LEN

"""# Preprocess data"""

source_lengths = [len(tokenizer.encode(text)) for text in pd.DataFrame(train_ds)['Content']]
target_lengths = [len(tokenizer.encode(text)) for text in pd.DataFrame(train_ds)['Summary']]
max_source_tokens = max(source_lengths)
max_target_tokens = max(target_lengths)
print(f'{max_source_tokens=}')
print(f'{max_target_tokens=}')

# Plotting the distribution of token lengths
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.hist(source_lengths, bins=100, alpha=0.5, color='blue')
plt.title('Distribution of Token Lengths for Content')
plt.xlabel('Token Count')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(10, 6))
plt.hist(target_lengths, bins=100, alpha=0.5, color='blue')
plt.title('Distribution of Token Lengths for Summary')
plt.xlabel('Token Count')
plt.ylabel('Frequency')
plt.show()

train_ds = train_ds.filter(filter_by_text_length_content)
eval_ds = eval_ds.filter(filter_by_text_length_content)

filtered_target_lengths_train = [len(tokenizer.encode(text)) for text in pd.DataFrame(train_ds)['Content']]
filtered_target_lengths_eval = [len(tokenizer.encode(text)) for text in pd.DataFrame(eval_ds)['Content']]

print(max(filtered_target_lengths_train))
print(max(filtered_target_lengths_eval))

plt.figure(figsize=(10, 6))
plt.hist(filtered_target_lengths_train, bins=100, alpha=0.5, color='blue')
plt.title('Distribution of Token Lengths for Content')
plt.xlabel('Token Count')
plt.ylabel('Frequency')
plt.show()

print(train_ds, eval_ds)

train_ds = train_ds.map(get_prompt)
val_ds = eval_ds.map(get_prompt)

val_ds.to_pandas()

tokenizer.eos_token_id

del filtered_target_lengths_train, filtered_target_lengths_eval, source_lengths, target_lengths

"""# Set up LoRa"""

model_auto.config.use_cache = False

model_auto.config.quantization_config.to_dict()

def print_trainable_parameters(model):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"
    )

print_trainable_parameters(model_auto)

from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16,
    lora_alpha=64,
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
        "lm_head",
    ],
    bias="none",
    lora_dropout=0.05,
    task_type="CAUSAL_LM",
)

# from peft import prepare_model_for_kbit_training

# model_auto.gradient_checkpointing_enable()
# model_lora = prepare_model_for_kbit_training(model_auto)

# model_lora = get_peft_model(model_lora, lora_config)
# print_trainable_parameters(model_auto)
# print_trainable_parameters(model_lora)

# Apply the accelerator. You can comment this out to remove the accelerator.
# model = accelerator.prepare_model(model)

"""## View model"""

print(model_auto)

"""# Wandb"""

import wandb, os
wandb.login()

wandb_project = "rosummary-llama2-finetune"
if len(wandb_project) > 0:
    os.environ["WANDB_PROJECT"] = wandb_project

API_KEY='0529a3ad694a5487302b30b185aebe0c349aa1da'

"""# Eval metrics"""

# Setup evaluation
import nltk
import evaluate
from datasets import load_metric
import numpy as np
nltk.download("punkt", quiet=True)

metric_rouge = load_metric("rouge")
metric_bleu = load_metric("bleu")

# metric = evaluate.load("rouge")

# def compute_metrics(eval_preds):
#     preds, labels = eval_preds

#     # decode preds and labels
#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
#     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

#     # rougeLSum expects newline after each sentence
#     decoded_preds = ["\n".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]
#     decoded_labels = ["\n".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]

#     result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
#     return result

"""# Trainer train"""

import transformers
from datetime import datetime

if torch.cuda.device_count() > 1: # If more than 1 GPU
    model_auto.is_parallelizable = True
    model_auto.model_parallel = True

model_auto.config.use_cache = False

train_ds

output_dir

train_ds_4sft = train_ds.map(lambda x: {"text": x["text"]+x["Summary"]})
val_ds_4sft = val_ds.map(lambda x: {"text": x["text"]+x["Summary"]})

train_ds_4sft.to_pandas().iloc[0]['text']

trainer = SFTTrainer(
    model=model_auto,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    peft_config=lora_config,
    dataset_text_field="text",
    max_seq_length=2*config.MAX_SOURCE_LEN+128,
    tokenizer=tokenizer,
    # packing=True,
    args=transformers.TrainingArguments(
                  per_device_train_batch_size=4,
                  gradient_accumulation_steps=4,
                  optim="paged_adamw_32bit",
                  logging_steps=1,
                  learning_rate=config_model['lr'],
                  fp16=True,
                  max_grad_norm=0.3,
                  num_train_epochs=config_model['epochs'],
                  evaluation_strategy="epoch",
                  eval_steps=0.2,
                  warmup_ratio=0.05,
                  save_strategy="epoch",
                  group_by_length=True,
                  output_dir=output_dir,
                  report_to="wandb",
                  save_safetensors=True,
                  load_best_model_at_end = True,
                  greater_is_better=False,
                  metric_for_best_model="eval_loss",
                  # lr_scheduler_type="cosine",
                  seed=42,
                  run_name=f"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}"
        )
)

trainer.train()

trainer.save_model()

"""# Eval"""

output_dir ='/content/gdrive/MyDrive/Colab Notebooks/automated_journalism/llama_models/meta-llama/Llama-2-7b-chat-hf-rosummary_4'

output_dir

# from peft import PeftModel

# model_name = config.MODEL_NAME
# adapters_name = output_dir

# print(f"Starting to load the model {model_name} into memory")

# m = AutoModelForCausalLM.from_pretrained(
#     model_name,
#     torch_dtype=torch.bfloat16,
#     device_map="auto"
# )
# m = PeftModel.from_pretrained(m, adapters_name)
# m = m.merge_and_unload()

bnb_config = BitsAndBytesConfig(
      load_in_4bit=True,
      bnb_4bit_quant_type="nf4",
      bnb_4bit_compute_dtype=torch.float16,
  )
tokenizer = AutoTokenizer.from_pretrained(
    config.MODEL_NAME,
    padding_side="right",
    add_eos_token=True,
    add_bos_token=True,
    use_fast=True
)
tokenizer.pad_token = tokenizer.eos_token
model_trained = AutoModelForCausalLM.from_pretrained(output_dir,
                                             device_map="cpu",
                                             trust_remote_code=True,
                                              # quantization_config=bnb_config,
                                            )

model_trained.config

test_ds = load_dataset(config.DATASET_NAME, split="test[:2000]")
test_ds = test_ds.remove_columns(["Title","href","Source","Category"])

# We drop instances with more than 512 tokens for GOOD memory usage
def filter_by_text_length_content(data):
  return len(tokenizer.encode(data['Content'])) < config.MAX_SOURCE_LEN

test_ds = test_ds.filter(filter_by_text_length_content)

import re
def clean_text(text):
  text = re.sub(r"http\S+", "", text)
  text = re.sub(r"@[^\s]+", "", text)
  text = re.sub(r"\s+", " ", text)
  return re.sub(r"\^[^ ]+", "", text)


def get_prompt(data):
  prompt = config.PROMPT
  clean_data = clean_text(data['Content'])
  data['text'] = prompt.format(article=clean_data)
  return data

test_ds = test_ds.map(get_prompt)

test_ds.to_pandas()

text = test_ds['text'][:1][0]
print(text)

from transformers import GenerationConfig
gen_config = GenerationConfig.from_pretrained(config.MODEL_NAME)

gen_config

tokenized_prompt = tokenizer(text, return_tensors='pt')['input_ids']
with torch.no_grad():
  output = model_trained.generate(
      tokenized_prompt,
      generation_config= gen_config,
      # top_p = config_model['top_p'],
      # top_k = config_model['top_k'],
      # temperature = config_model['temperature'],
      max_new_tokens = 2*config.MAX_TARGET_LEN
  )

decoded_output = tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)
print(decoded_output)

decoded_output = tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)
print(decoded_output)

decoded_output = tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)
print(decoded_output)

decoded_output = tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)
print(decoded_output)

decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)
print(decoded_output)

model_trained_qnt = AutoModelForCausalLM.from_pretrained(output_dir,
                                             device_map="auto",
                                             trust_remote_code=True,
                                              quantization_config=bnb_config,
                                            )

tokenized_prompt = tokenizer(text, return_tensors='pt')['input_ids'].cuda()
with torch.no_grad():
  output = model_trained_qnt.generate(
      tokenized_prompt,
      generation_config= gen_config,
      top_p = config_model['top_p'],
      top_k = config_model['top_k'],
      temperature = config_model['temperature'],
      max_new_tokens = 2*config.MAX_TARGET_LEN
  )

decoded_output = tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)
print(decoded_output)

decoded_output = tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)
print(decoded_output)